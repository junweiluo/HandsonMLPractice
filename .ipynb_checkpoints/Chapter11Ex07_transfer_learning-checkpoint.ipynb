{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junweiluo/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/junweiluo/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/junweiluo/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/junweiluo/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/junweiluo/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/junweiluo/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "# model A will be multiclass classification while model B will be binary classfication (for different stuffs)\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "# softmax is for multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 408       \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# since model A is multiclassfication/softmax, it uses sparse categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/20\n",
      "43986/43986 [==============================] - 4s 90us/sample - loss: 0.5641 - accuracy: 0.8191 - val_loss: 0.3760 - val_accuracy: 0.8719\n",
      "Epoch 2/20\n",
      "43986/43986 [==============================] - 3s 71us/sample - loss: 0.3529 - accuracy: 0.8781 - val_loss: 0.3196 - val_accuracy: 0.8919\n",
      "Epoch 3/20\n",
      "43986/43986 [==============================] - 3s 69us/sample - loss: 0.3155 - accuracy: 0.8902 - val_loss: 0.2958 - val_accuracy: 0.9001\n",
      "Epoch 4/20\n",
      "43986/43986 [==============================] - 3s 70us/sample - loss: 0.2961 - accuracy: 0.8971 - val_loss: 0.2860 - val_accuracy: 0.9008\n",
      "Epoch 5/20\n",
      "43986/43986 [==============================] - 3s 71us/sample - loss: 0.2834 - accuracy: 0.9017 - val_loss: 0.2717 - val_accuracy: 0.9101\n",
      "Epoch 6/20\n",
      "43986/43986 [==============================] - 3s 71us/sample - loss: 0.2738 - accuracy: 0.9060 - val_loss: 0.2692 - val_accuracy: 0.9106\n",
      "Epoch 7/20\n",
      "43986/43986 [==============================] - 3s 69us/sample - loss: 0.2662 - accuracy: 0.9089 - val_loss: 0.2652 - val_accuracy: 0.91032655 \n",
      "Epoch 8/20\n",
      "43986/43986 [==============================] - 3s 71us/sample - loss: 0.2591 - accuracy: 0.9107 - val_loss: 0.2582 - val_accuracy: 0.9111\n",
      "Epoch 9/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2535 - accuracy: 0.9145 - val_loss: 0.2586 - val_accuracy: 0.9128\n",
      "Epoch 10/20\n",
      "43986/43986 [==============================] - 3s 70us/sample - loss: 0.2486 - accuracy: 0.9151 - val_loss: 0.2608 - val_accuracy: 0.9128\n",
      "Epoch 11/20\n",
      "43986/43986 [==============================] - 3s 70us/sample - loss: 0.2440 - accuracy: 0.9166 - val_loss: 0.2504 - val_accuracy: 0.9170\n",
      "Epoch 12/20\n",
      "43986/43986 [==============================] - 3s 72us/sample - loss: 0.2400 - accuracy: 0.9185 - val_loss: 0.2499 - val_accuracy: 0.9148\n",
      "Epoch 13/20\n",
      "43986/43986 [==============================] - 3s 70us/sample - loss: 0.2363 - accuracy: 0.9192 - val_loss: 0.2450 - val_accuracy: 0.9153\n",
      "Epoch 14/20\n",
      "43986/43986 [==============================] - 3s 70us/sample - loss: 0.2332 - accuracy: 0.9200 - val_loss: 0.2439 - val_accuracy: 0.9168\n",
      "Epoch 15/20\n",
      "43986/43986 [==============================] - 3s 77us/sample - loss: 0.2295 - accuracy: 0.9221 - val_loss: 0.2393 - val_accuracy: 0.9170\n",
      "Epoch 16/20\n",
      "43986/43986 [==============================] - 4s 93us/sample - loss: 0.2263 - accuracy: 0.9222 - val_loss: 0.2436 - val_accuracy: 0.9188\n",
      "Epoch 17/20\n",
      "43986/43986 [==============================] - 4s 89us/sample - loss: 0.2242 - accuracy: 0.9233 - val_loss: 0.2444 - val_accuracy: 0.9145\n",
      "Epoch 18/20\n",
      "43986/43986 [==============================] - 3s 77us/sample - loss: 0.2215 - accuracy: 0.9244 - val_loss: 0.2416 - val_accuracy: 0.9131\n",
      "Epoch 19/20\n",
      "43986/43986 [==============================] - 3s 73us/sample - loss: 0.2188 - accuracy: 0.9248 - val_loss: 0.2362 - val_accuracy: 0.9155\n",
      "Epoch 20/20\n",
      "43986/43986 [==============================] - 3s 77us/sample - loss: 0.2166 - accuracy: 0.9251 - val_loss: 0.2330 - val_accuracy: 0.9190\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to clone a model, so when running the new model, it won't impact the old one.\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can load any layers as desired\n",
    "\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the old layers non-trainable\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the whole model back into a model, then remove certain layers to another new. then add more layers.\n",
    "\n",
    "How to just load weights?  Configurations?\n",
    "How to transfer learning for Functional model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lock the layers transfered from original and train.\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.9988 - accuracy: 0.5050 - val_loss: 0.9127 - val_accuracy: 0.5284\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 359us/sample - loss: 0.9370 - accuracy: 0.5050 - val_loss: 0.8564 - val_accuracy: 0.5436\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 288us/sample - loss: 0.8780 - accuracy: 0.5150 - val_loss: 0.8063 - val_accuracy: 0.5568\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 304us/sample - loss: 0.8256 - accuracy: 0.5400 - val_loss: 0.7584 - val_accuracy: 0.5740\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 0s 2ms/sample - loss: 0.6411 - accuracy: 0.6100 - val_loss: 0.4491 - val_accuracy: 0.7809\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 0s 335us/sample - loss: 0.3867 - accuracy: 0.8350 - val_loss: 0.3163 - val_accuracy: 0.8864\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 0s 329us/sample - loss: 0.2741 - accuracy: 0.9100 - val_loss: 0.2482 - val_accuracy: 0.9189\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 0s 330us/sample - loss: 0.2145 - accuracy: 0.9500 - val_loss: 0.2047 - val_accuracy: 0.9391\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 0s 326us/sample - loss: 0.1764 - accuracy: 0.9600 - val_loss: 0.1745 - val_accuracy: 0.9533\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 0s 324us/sample - loss: 0.1484 - accuracy: 0.9700 - val_loss: 0.1533 - val_accuracy: 0.9625\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 0s 349us/sample - loss: 0.1296 - accuracy: 0.9800 - val_loss: 0.1385 - val_accuracy: 0.9675\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 0s 368us/sample - loss: 0.1157 - accuracy: 0.9800 - val_loss: 0.1251 - val_accuracy: 0.9696\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 0s 321us/sample - loss: 0.1036 - accuracy: 0.9800 - val_loss: 0.1151 - val_accuracy: 0.9726\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 0s 324us/sample - loss: 0.0942 - accuracy: 0.9800 - val_loss: 0.1059 - val_accuracy: 0.9787\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 0s 322us/sample - loss: 0.0861 - accuracy: 0.9800 - val_loss: 0.0980 - val_accuracy: 0.9817\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 0s 309us/sample - loss: 0.0790 - accuracy: 0.9850 - val_loss: 0.0921 - val_accuracy: 0.9858\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 0s 372us/sample - loss: 0.0736 - accuracy: 0.9900 - val_loss: 0.0866 - val_accuracy: 0.9858\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 0s 349us/sample - loss: 0.0682 - accuracy: 0.9900 - val_loss: 0.0819 - val_accuracy: 0.9858\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 0s 334us/sample - loss: 0.0638 - accuracy: 0.9900 - val_loss: 0.0779 - val_accuracy: 0.9858\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 0s 328us/sample - loss: 0.0600 - accuracy: 0.9900 - val_loss: 0.0734 - val_accuracy: 0.9868\n"
     ]
    }
   ],
   "source": [
    "# Then open the transferred layrs for training.\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# need to re-complie.\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "# train for more epochs.\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
